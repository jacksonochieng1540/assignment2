
"""
Comprehensive Performance Evaluation Script - FINAL FIXED VERSION
"""

import sys
import os
import time
import json
import random
import threading
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from typing import Dict, List, Any

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# Import modules
try:
    from shared.protocols.transactions import TransactionCoordinator
    from shared.protocols.fault_tolerance import ReplicationManager
    from edge.services.edge_node import EdgeNode
    from core.services.core_node import CoreNode
    from cloud.services.cloud_node import CloudNode
    print("‚úÖ All imports successful!")
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    sys.exit(1)

class PerformanceEvaluator:
    """Evaluates system performance under various conditions"""
    
    def __init__(self, output_dir: str = "results"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(f"{output_dir}/graphs", exist_ok=True)
        os.makedirs(f"{output_dir}/tables", exist_ok=True)
        os.makedirs(f"{output_dir}/reports", exist_ok=True)
        
        self.results = {
            "latency_tests": {},
            "throughput_tests": {},
            "transaction_tests": {},
            "fault_tolerance_tests": {}
        }
    
    def evaluate_latency(self, nodes: List, num_requests: int = 50) -> Dict[str, Any]:
        """Evaluate latency across different node types"""
        print(f"\n{'='*50}")
        print("LATENCY EVALUATION")
        print(f"{'='*50}")
        
        results = {}
        
        for node in nodes:
            node_type = node.__class__.__name__
            latencies = []
            
            print(f"\nTesting {node.node_id} ({node_type})...")
            
            for i in range(num_requests):
                # Create a simple request based on node type
                if node_type == "EdgeNode":
                    request = {"type": "cache", "key": f"key-{i}", "operation": "get"}
                elif node_type == "CoreNode":
                    request = {"type": "session", "session_id": f"session-{i}", "action": "validate"}
                else:  # CloudNode
                    request = {"type": "analytics", "job_id": f"job-{i}", "data": f"data-{i}"}
                
                start = time.time()
                try:
                    # Call process_request if it exists
                    if hasattr(node, 'process_request'):
                        response = node.process_request(request)
                    else:
                        # Simulate processing
                        time.sleep(random.uniform(0.001, 0.1))
                except Exception as e:
                    print(f"  Warning on request {i}: {e}")
                    continue
                    
                latency = (time.time() - start) * 1000  # ms
                latencies.append(latency)
            
            if latencies:
                results[node.node_id] = {
                    "node_type": node_type,
                    "min_latency_ms": min(latencies),
                    "max_latency_ms": max(latencies),
                    "avg_latency_ms": np.mean(latencies),
                    "median_latency_ms": np.median(latencies),
                    "p95_latency_ms": np.percentile(latencies, 95),
                    "p99_latency_ms": np.percentile(latencies, 99),
                    "std_dev_ms": np.std(latencies)
                }
                
                print(f"  Avg latency: {results[node.node_id]['avg_latency_ms']:.2f}ms")
                print(f"  P95 latency: {results[node.node_id]['p95_latency_ms']:.2f}ms")
        
        self.results["latency_tests"] = results
        self._plot_latency_comparison(results)
        self._save_latency_table(results)
        
        return results
    
    def evaluate_throughput(self, nodes: List, duration_seconds: int = 5) -> Dict[str, Any]:
        """Evaluate throughput (requests per second)"""
        print(f"\n{'='*50}")
        print("THROUGHPUT EVALUATION")
        print(f"{'='*50}")
        
        results = {}
        
        for node in nodes:
            print(f"\nTesting {node.node_id} throughput...")
            
            request_count = 0
            errors = 0
            start_time = time.time()
            end_time = start_time + duration_seconds
            
            def worker():
                nonlocal request_count, errors
                while time.time() < end_time:
                    try:
                        request = {"type": "simple", "operation": "ping"}
                        if hasattr(node, 'process_request'):
                            node.process_request(request)
                        request_count += 1
                    except Exception as e:
                        errors += 1
            
            # Use threads
            threads = []
            num_threads = min(4, os.cpu_count() or 2)
            for _ in range(num_threads):
                t = threading.Thread(target=worker)
                t.start()
                threads.append(t)
            
            for t in threads:
                t.join()
            
            actual_duration = time.time() - start_time
            throughput = request_count / actual_duration if actual_duration > 0 else 0
            
            results[node.node_id] = {
                "node_type": node.__class__.__name__,
                "duration_seconds": actual_duration,
                "total_requests": request_count,
                "errors": errors,
                "throughput_rps": throughput,
                "error_rate_percent": (errors / request_count * 100) if request_count > 0 else 0
            }
            
            print(f"  Throughput: {throughput:.2f} requests/second")
            print(f"  Total requests: {request_count}")
        
        self.results["throughput_tests"] = results
        self._plot_throughput_comparison(results)
        self._save_throughput_table(results)
        
        return results
    
    def evaluate_transactions(self, coordinator: TransactionCoordinator, 
                            num_transactions: int = 20) -> Dict[str, Any]:
        """Evaluate transaction performance"""
        print(f"\n{'='*50}")
        print("TRANSACTION EVALUATION")
        print(f"{'='*50}")
        
        results = {
            "2pc": {"latencies": [], "success": 0, "failed": 0},
            "3pc": {"latencies": [], "success": 0, "failed": 0}
        }
        
        participants = [f"participant-{i}" for i in range(5)]
        
        print(f"\nTesting 2PC protocol...")
        for i in range(num_transactions // 2):
            try:
                # FIX: Use begin_transaction instead of start_transaction
                txn_id = coordinator.begin_transaction(participants, {"op": f"2pc-test-{i}"})
                start = time.time()
                success = coordinator.execute_2pc(txn_id)
                latency = (time.time() - start) * 1000
                
                results["2pc"]["latencies"].append(latency)
                if success:
                    results["2pc"]["success"] += 1
                else:
                    results["2pc"]["failed"] += 1
            except Exception as e:
                print(f"  Warning in 2PC transaction {i}: {e}")
                results["2pc"]["failed"] += 1
        
        print(f"\nTesting 3PC protocol...")
        for i in range(num_transactions // 2):
            try:
                # FIX: Use begin_transaction instead of start_transaction
                txn_id = coordinator.begin_transaction(participants, {"op": f"3pc-test-{i}"})
                start = time.time()
                success = coordinator.execute_3pc(txn_id)
                latency = (time.time() - start) * 1000
                
                results["3pc"]["latencies"].append(latency)
                if success:
                    results["3pc"]["success"] += 1
                else:
                    results["3pc"]["failed"] += 1
            except Exception as e:
                print(f"  Warning in 3PC transaction {i}: {e}")
                results["3pc"]["failed"] += 1
        
        # Calculate statistics
        for protocol in ["2pc", "3pc"]:
            latencies = results[protocol]["latencies"]
            if latencies:
                results[protocol]["avg_latency_ms"] = np.mean(latencies)
                results[protocol]["p95_latency_ms"] = np.percentile(latencies, 95)
                total = results[protocol]["success"] + results[protocol]["failed"]
                results[protocol]["success_rate"] = (results[protocol]["success"] / total * 100) if total > 0 else 0
                
                print(f"\n{protocol.upper()} Results:")
                print(f"  Success rate: {results[protocol]['success_rate']:.2f}%")
                print(f"  Avg latency: {results[protocol]['avg_latency_ms']:.2f}ms")
            else:
                print(f"\n{protocol.upper()}: No successful transactions")
        
        self.results["transaction_tests"] = results
        if results["2pc"]["latencies"] or results["3pc"]["latencies"]:
            self._plot_transaction_comparison(results)
        
        return results
    
    def evaluate_fault_tolerance(self, replication_manager: ReplicationManager) -> Dict[str, Any]:
        """Evaluate fault tolerance and recovery"""
        print(f"\n{'='*50}")
        print("FAULT TOLERANCE EVALUATION")
        print(f"{'='*50}")
        
        results = {
            "before_failure": {"active_nodes": 0, "replicas": 0},
            "after_failure": {"active_nodes": 0, "replicas": 0},
            "recovery_time_ms": 0
        }
        
        # Add nodes
        all_nodes = [f"node-{i}" for i in range(6)]
        for node in all_nodes:
            replication_manager.add_node(node)
        
        # Create replicas
        replicas_created = 0
        for i in range(10):
            if hasattr(replication_manager, 'replicate_data'):
                if replication_manager.replicate_data(f"data-{i}", f"value-{i}"):
                    replicas_created += 3
        
        results["before_failure"]["active_nodes"] = len(all_nodes)
        results["before_failure"]["replicas"] = replicas_created
        print(f"\nBefore failure: {len(all_nodes)} nodes, {replicas_created} replicas")
        
        # Simulate failures
        print("\nSimulating node failures...")
        failed_nodes = ["node-0", "node-1"]
        
        recovery_start = time.time()
        for node in failed_nodes:
            if hasattr(replication_manager, 'simulate_node_failure'):
                replication_manager.simulate_node_failure(node)
        
        # Count active nodes
        active_nodes = len(all_nodes) - len(failed_nodes)
        
        recovery_time = (time.time() - recovery_start) * 1000
        results["recovery_time_ms"] = recovery_time
        results["after_failure"]["active_nodes"] = active_nodes
        results["after_failure"]["replicas"] = replicas_created // 2  # Rough estimate
        
        print(f"After failure: {active_nodes} active nodes")
        print(f"Recovery time: {recovery_time:.2f}ms")
        
        self.results["fault_tolerance_tests"] = results
        self._plot_fault_tolerance(results)
        
        return results
    
    def _plot_latency_comparison(self, results: Dict[str, Any]):
        """Plot latency comparison"""
        try:
            if not results:
                return
                
            nodes = list(results.keys())
            avg_latencies = [results[node]["avg_latency_ms"] for node in nodes]
            
            fig, ax = plt.subplots(figsize=(10, 6))
            colors = ['skyblue', 'lightcoral', 'lightgreen']
            bars = ax.bar(nodes, avg_latencies, color=colors[:len(nodes)])
            
            ax.set_xlabel('Nodes')
            ax.set_ylabel('Average Latency (ms)')
            ax.set_title('Latency Comparison Across Nodes')
            
            # Add value labels on bars
            for bar, latency in zip(bars, avg_latencies):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                       f'{latency:.1f}ms', ha='center', va='bottom')
            
            ax.grid(axis='y', alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/graphs/latency_comparison.png", dpi=150)
            plt.close()
            
            print(f"‚úì Latency graph saved")
        except Exception as e:
            print(f"Warning: Could not create latency graph: {e}")
    
    def _plot_throughput_comparison(self, results: Dict[str, Any]):
        """Plot throughput comparison"""
        try:
            if not results:
                return
                
            nodes = list(results.keys())
            throughputs = [results[node]["throughput_rps"] for node in nodes]
            
            fig, ax = plt.subplots(figsize=(10, 6))
            colors = ['mediumseagreen', 'darkorange', 'mediumpurple']
            bars = ax.bar(nodes, throughputs, color=colors[:len(nodes)])
            
            ax.set_xlabel('Nodes')
            ax.set_ylabel('Throughput (requests/second)')
            ax.set_title('Throughput Comparison Across Nodes')
            
            # Add value labels
            for bar, throughput in zip(bars, throughputs):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                       f'{throughput:.1f} rps', ha='center', va='bottom')
            
            ax.grid(axis='y', alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/graphs/throughput_comparison.png", dpi=150)
            plt.close()
            
            print(f"‚úì Throughput graph saved")
        except Exception as e:
            print(f"Warning: Could not create throughput graph: {e}")
    
    def _plot_transaction_comparison(self, results: Dict[str, Any]):
        """Plot transaction protocol comparison"""
        try:
            protocols = ["2pc", "3pc"]
            success_rates = [results[p].get("success_rate", 0) for p in protocols]
            
            fig, ax = plt.subplots(figsize=(8, 6))
            bars = ax.bar(protocols, success_rates, color=['steelblue', 'darkorange'])
            
            ax.set_xlabel('Protocol')
            ax.set_ylabel('Success Rate (%)')
            ax.set_title('Transaction Protocol Success Rates')
            ax.set_ylim([0, 105])
            
            # Add value labels
            for bar, rate in zip(bars, success_rates):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                       f'{rate:.1f}%', ha='center', va='bottom')
            
            ax.grid(axis='y', alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/graphs/transaction_comparison.png", dpi=150)
            plt.close()
            
            print(f"‚úì Transaction graph saved")
        except Exception as e:
            print(f"Warning: Could not create transaction graph: {e}")
    
    def _plot_fault_tolerance(self, results: Dict[str, Any]):
        """Plot fault tolerance metrics"""
        try:
            stages = ['Before Failure', 'After Failure']
            active_nodes = [
                results["before_failure"]["active_nodes"],
                results["after_failure"]["active_nodes"]
            ]
            
            fig, ax = plt.subplots(figsize=(8, 6))
            bars = ax.bar(stages, active_nodes, color=['green', 'orange'])
            
            ax.set_xlabel('Stage')
            ax.set_ylabel('Active Nodes')
            ax.set_title('Fault Tolerance: Active Nodes')
            
            # Add value labels
            for bar, count in zip(bars, active_nodes):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                       f'{count}', ha='center', va='bottom')
            
            ax.grid(axis='y', alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/graphs/fault_tolerance.png", dpi=150)
            plt.close()
            
            print(f"‚úì Fault tolerance graph saved")
        except Exception as e:
            print(f"Warning: Could not create fault tolerance graph: {e}")
    
    def _save_latency_table(self, results: Dict[str, Any]):
        """Save latency results to CSV"""
        try:
            if not results:
                return
                
            data = []
            for node, metrics in results.items():
                data.append({
                    "Node": node,
                    "Node Type": metrics["node_type"],
                    "Min (ms)": f"{metrics['min_latency_ms']:.2f}",
                    "Max (ms)": f"{metrics['max_latency_ms']:.2f}",
                    "Avg (ms)": f"{metrics['avg_latency_ms']:.2f}",
                    "Median (ms)": f"{metrics['median_latency_ms']:.2f}",
                    "P95 (ms)": f"{metrics['p95_latency_ms']:.2f}"
                })
            
            df = pd.DataFrame(data)
            df.to_csv(f"{self.output_dir}/tables/latency_results.csv", index=False)
            print(f"‚úì Latency table saved")
        except Exception as e:
            print(f"Warning: Could not save latency table: {e}")
    
    def _save_throughput_table(self, results: Dict[str, Any]):
        """Save throughput results to CSV"""
        try:
            if not results:
                return
                
            data = []
            for node, metrics in results.items():
                data.append({
                    "Node": node,
                    "Node Type": metrics["node_type"],
                    "Duration (s)": f"{metrics['duration_seconds']:.2f}",
                    "Total Requests": metrics["total_requests"],
                    "Throughput (req/s)": f"{metrics['throughput_rps']:.2f}"
                })
            
            df = pd.DataFrame(data)
            df.to_csv(f"{self.output_dir}/tables/throughput_results.csv", index=False)
            print(f"‚úì Throughput table saved")
        except Exception as e:
            print(f"Warning: Could not save throughput table: {e}")
    
    def generate_final_report(self):
        """Generate final comprehensive report"""
        print(f"\n{'='*50}")
        print("GENERATING FINAL REPORT")
        print(f"{'='*50}")
        
        try:
            with open(f"{self.output_dir}/reports/performance_report.json", 'w') as f:
                json.dump(self.results, f, indent=2)
            
            print(f"‚úì Final report saved")
            
            # Print summary
            print("\nüìä PERFORMANCE SUMMARY:")
            print("-" * 40)
            
            if "latency_tests" in self.results and self.results["latency_tests"]:
                print("Latency (average):")
                for node, metrics in self.results["latency_tests"].items():
                    print(f"  {node}: {metrics['avg_latency_ms']:.2f}ms")
            
            if "throughput_tests" in self.results and self.results["throughput_tests"]:
                print("\nThroughput:")
                for node, metrics in self.results["throughput_tests"].items():
                    print(f"  {node}: {metrics['throughput_rps']:.1f} req/s")
            
            if "fault_tolerance_tests" in self.results:
                ft = self.results["fault_tolerance_tests"]
                print(f"\nFault Recovery: {ft.get('recovery_time_ms', 0):.1f}ms")
            
            print(f"\n‚úÖ All results saved to '{self.output_dir}/' directory")
            
        except Exception as e:
            print(f"Warning: Could not generate final report: {e}")

def main():
    """Main evaluation entry point"""
    print("="*70)
    print("DISTRIBUTED TELECOM SYSTEM - PERFORMANCE EVALUATION")
    print("="*70)
    
    # Create evaluator
    evaluator = PerformanceEvaluator()
    
    # Create test nodes
    print("\nInitializing test nodes...")
    
    try:
        edge_node = EdgeNode("edge-perf", "region-test", ["core-perf"])
        core_node = CoreNode("core-perf", "region-test", ["edge-perf"], ["cloud-perf"])
        cloud_node = CloudNode("cloud-perf", "dc-test")
        
        print("‚úì Nodes created")
        
        # Start nodes if they have start method
        for node in [edge_node, core_node, cloud_node]:
            if hasattr(node, 'start'):
                node.start(num_workers=2)
        
        print("‚úì Nodes started")
        time.sleep(1)
        
        try:
            # Run evaluations
            evaluator.evaluate_latency([edge_node, core_node, cloud_node], num_requests=30)
            evaluator.evaluate_throughput([edge_node, core_node, cloud_node], duration_seconds=3)
            
            # Check if transaction methods exist
            coordinator = TransactionCoordinator("eval-coordinator")
            if hasattr(coordinator, 'begin_transaction'):
                evaluator.evaluate_transactions(coordinator, num_transactions=10)
            else:
                print("\n‚ö† Transaction evaluation skipped (methods not found)")
            
            # Check if replication methods exist
            replication_manager = ReplicationManager(replication_factor=3)
            if hasattr(replication_manager, 'add_node'):
                evaluator.evaluate_fault_tolerance(replication_manager)
            else:
                print("\n‚ö† Fault tolerance evaluation skipped (methods not found)")
            
            # Generate final report
            evaluator.generate_final_report()
            
        finally:
            # Cleanup
            print("\nCleaning up...")
            for node in [edge_node, core_node, cloud_node]:
                if hasattr(node, 'stop'):
                    node.stop()
            print("‚úì Nodes stopped")
    
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print("\n" + "="*70)
    print("‚úÖ EVALUATION COMPLETE!")
    print("="*70)
    print("\nüéâ Your system is working! Check the 'results/' directory for outputs.")

if __name__ == "__main__":
    main()
